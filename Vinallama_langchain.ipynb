{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "gw57pbZ594RS"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrinhNhatTuyen/RAGwithLangchainVinaLlama/blob/main/Vinallama_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tham khảo https://youtu.be/lQ1eevHm0C4?si=OpGzYJ8OQQ4xN7Bm"
      ],
      "metadata": {
        "id": "R7T-Msvs7vVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate \\\n",
        "bitsandbytes \\\n",
        "langchain \\\n",
        "huggingface_hub \\\n",
        "langchain_core \\\n",
        "langchain_community \\\n",
        "langchain-huggingface \\\n",
        "pypdf \\\n",
        "langchain_chroma \\\n",
        "chromadb \\\n",
        "gpt4all \\\n",
        "faiss-gpu \\\n",
        "openai \\\n",
        "cohere \\\n",
        "langchain_cohere\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "UM5OfQgdAE7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load model**"
      ],
      "metadata": {
        "id": "vCM4QQnT6OUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\")\n",
        "\n",
        "model_name = \"vilm/vinallama-2.7b-chat\"\n",
        "access_token = \"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\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token)\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "_Atyzt7DTwgW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformers pipeline**"
      ],
      "metadata": {
        "id": "LFcchEpW6QkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline = pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task='text-generation',\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "VIv4vidfT-Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_kwargs = {\n",
        "    \"temperature\": 0.1\n",
        "}\n",
        "\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=model_pipeline,\n",
        "    model_kwargs=gen_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "A8B1BIVIDTp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Template của VinaLlama**"
      ],
      "metadata": {
        "id": "imZIb20Q6ZVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sử dụng thông tin sau để tạo câu trả lời. Nếu bạn không biết câu trả lời, đừng cố tạo câu trả lời.\n",
        "\n",
        "template = \"\"\"\n",
        "<|im_start|>system\n",
        "Bạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QBZYr4RMBUFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(template=template,\n",
        "                                          input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "GR-VhHe_vqo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt Template**"
      ],
      "metadata": {
        "id": "BOG5qexl6FCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"thủ đô của canada là gì\"\n",
        "mss = prompt_template.format(question=user_prompt)"
      ],
      "metadata": {
        "id": "G791F-apBhwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# qna1 = llm.invoke(mss)"
      ],
      "metadata": {
        "id": "1G7P76ucynnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# answer1 = qna1.split(\"<|im_start|>assistant\")[1].split(\"<|im_end|>\")[0].strip()\n",
        "# print(answer1)"
      ],
      "metadata": {
        "id": "wABtIEcZ2H-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tạo chain**"
      ],
      "metadata": {
        "id": "kuRdGtVv57xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chain = prompt_template | llm"
      ],
      "metadata": {
        "id": "myEKTkGJyTcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# qna2 = chain.invoke({\"question\": \"giải thích dễ hiểu về đạo hàm\"})\n",
        "# answer2 = qna2.split(\"<|im_start|>assistant\")[1].split(\"<|im_end|>\")[0].strip()\n",
        "# print(answer2)"
      ],
      "metadata": {
        "id": "luUaBMakygEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output Parser**\n",
        "\n",
        "> đôi khi trong output k có chuỗi json\n",
        "\n"
      ],
      "metadata": {
        "id": "gw57pbZ594RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "class QnA (BaseModel):\n",
        "\n",
        "    question: str = Field(description=\"user question\")\n",
        "\n",
        "    answer: str = Field(description=\"answer to user question\")\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=QnA)"
      ],
      "metadata": {
        "id": "QoptCb1a63n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# thêm format_instructions để model biết mà định dạng câu trả lời\n",
        "template2 = \"\"\"\n",
        "<|im_start|>system\n",
        "Bạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\n",
        "{format_instructions}\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Hp33XPBX999G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template2 = PromptTemplate(\n",
        "    template=template2,\n",
        "    input_variables=[\"question\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")"
      ],
      "metadata": {
        "id": "tjVAn6c77qTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain2 = prompt_template2 | llm"
      ],
      "metadata": {
        "id": "wdA96Qwf8d8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna3 = chain2.invoke({\"question\": \"dân số việt nam là bao nhiêu\"})\n",
        "parser_output = parser.invoke(qna3)\n",
        "parser_output"
      ],
      "metadata": {
        "id": "_oHxDcm6BOab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e32c54-d5dd-4016-e75f-2d775d741708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'properties': {'question': {'title': 'Question',\n",
              "   'description': 'user question',\n",
              "   'type': 'string'},\n",
              "  'answer': {'title': 'Answer',\n",
              "   'description': 'answer to user question',\n",
              "   'type': 'string'}},\n",
              " 'required': ['question', 'answer']}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer3 = qna3.split(\"<|im_start|>assistant\")[1].split(\"<|im_end|>\")[0].strip()\n",
        "print(answer3)"
      ],
      "metadata": {
        "id": "5Bm9d6KX-lBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53cec8ca-ae90-4a5a-e885-a9e7cb3b98e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dân số của Việt Nam vào năm 2021 là 96,4 triệu người, theo dữ liệu từ Ngân hàng Thế giới.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(qna3.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGeoOOqUGYgx",
        "outputId": "e653e059-b2ae-41c9-b9eb-3f9450d45404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>system\n",
            "Bạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"question\": {\"title\": \"Question\", \"description\": \"user question\", \"type\": \"string\"}, \"answer\": {\"title\": \"Answer\", \"description\": \"answer to user question\", \"type\": \"string\"}}, \"required\": [\"question\", \"answer\"]}\n",
            "```\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "dân số việt nam là bao nhiêu<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Dân số của Việt Nam vào năm 2021 là 96,4 triệu người, theo dữ liệu từ Ngân hàng Thế giới.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load PDF Files**"
      ],
      "metadata": {
        "id": "dHbjXWkoDYbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load từ link"
      ],
      "metadata": {
        "id": "ai0_IZqcE1gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    DirectoryLoader,\n",
        "    PyPDFDirectoryLoader\n",
        ")"
      ],
      "metadata": {
        "id": "m1w804t4IHlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local file or online file\n",
        "# 16 Pages\n",
        "# url = \"https://arxiv.org/pdf/2312.16862.pdf\"\n",
        "# pdf_loader = PyPDFLoader(url) # option: extract_images = True\n",
        "\n",
        "# docs = pdf_loader.load()"
      ],
      "metadata": {
        "id": "Uak49fPEEygG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải folder chứ file pdf từ drive\n",
        "import gdown, zipfile, os\n",
        "\n",
        "# URL của thư mục Google Drive\n",
        "url = \"https://drive.google.com/drive/folders/1sCJFUuRhs0ksqWya513uYseYo_voZg0V?usp=sharing\"\n",
        "\n",
        "# Tải folder từ Google Drive\n",
        "gdown.download_folder(url, quiet=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mgI88gK4E9JF",
        "outputId": "2a1772e1-aa8a-4da5-ff49-29add7e42df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/data/[Description]-Multimodal-Large-Language-Models.pdf',\n",
              " '/content/data/[Description]-RLHF.pdf',\n",
              " '/content/data/AIO2024_Module01_Project_RAG_Description.pdf',\n",
              " '/content/data/Description - MultiAgentLLMs.pdf',\n",
              " '/content/data/LangChain_Description.pdf',\n",
              " '/content/data/LLMs_Math_Solver_Instruction_Tuning.pdf',\n",
              " '/content/data/Prompt_Engineering_Guide.pdf',\n",
              " '/content/data/QA_Description.pdf',\n",
              " '/content/data/RAG.pdf',\n",
              " '/content/data/Yolov10.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load từ local"
      ],
      "metadata": {
        "id": "BcoqSyv-E4r2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load tùy loại file (.md, .pdf, .csv,...)"
      ],
      "metadata": {
        "id": "xlhn4LUCItOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pdf_data_path = \"/content/data\"\n",
        "# loader = DirectoryLoader(pdf_data_path, glob=\"*.pdf\", loader_cls=PyPDFLoader, show_progress=True)\n",
        "# documents = loader.load()"
      ],
      "metadata": {
        "id": "TXMXrAZv5BEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load file PDF từ folder"
      ],
      "metadata": {
        "id": "cm9t98nzJHIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_data_path = \"/content/data\"\n",
        "loader = PyPDFDirectoryLoader(pdf_data_path)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "6hoHWR1AH6yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pdf_loader = PyPDFLoader('/content/data/RAG.pdf') # option: extract_images = True\n",
        "# documents = pdf_loader.load()"
      ],
      "metadata": {
        "id": "Bm0wycJiDPzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Documents Splitter**"
      ],
      "metadata": {
        "id": "ia_02k98Kg2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RecursiveCharacterTextSplitter** và **CharacterTextSplitter** đều là các lớp trong Langchain dùng để chia văn bản thành các đoạn nhỏ hơn. Tuy nhiên, chúng có sự khác biệt trong cách thức hoạt động:\n",
        "\n",
        "*   **CharacterTextSplitter**: Chia văn\n",
        "bản dựa trên một ký tự phân cách duy nhất. Ví dụ, nếu bạn chỉ định dấu cách (\" \") làm ký tự phân cách, nó sẽ chia văn bản thành các từ riêng lẻ. Nó không đảm bảo kích thước đoạn văn bản.\n",
        "\n",
        "*   **RecursiveCharacterTextSplitter**: Chia văn bản một cách đệ quy dựa trên một danh sách các ký tự phân cách. Nó sẽ cố gắng chia văn bản theo thứ tự các ký tự phân cách trong danh sách cho đến khi các đoạn văn bản đủ nhỏ. Ưu điểm của phương pháp này là nó cố gắng giữ các đoạn văn bản có ý nghĩa liên quan với nhau, ví dụ như giữ nguyên cả câu hoặc đoạn văn.\n",
        "\n",
        "Tóm lại: **RecursiveCharacterTextSplitter** linh hoạt và thông minh hơn trong việc chia văn bản so với **CharacterTextSplitter**."
      ],
      "metadata": {
        "id": "YGjPaMcpM1PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter\n",
        ")\n",
        "\n",
        "chunk_size = 512\n",
        "chunk_overlap = 50\n",
        "separator=[\"\\n\", \"\\n\\n\", \".\"]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    separators=separator\n",
        ")"
      ],
      "metadata": {
        "id": "G4xzew3qLBL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split text"
      ],
      "metadata": {
        "id": "3wtW5qTbPso6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"\"\"\n",
        "Sáng 25/4, người dân xã Tân Tập, huyện Cần Giuộc (tỉnh Long An) tập trung rất đông tại khu vực phát nước miễn phí để nhận từng chai nước để phục vụ sinh hoạt.\n",
        "Xe chở bình nước tinh khiết loại 20 lít của nhóm giáo viên dạy lái xe đến từ Bình Dương len lỏi vào trong ấp Tân Chánh (xã Tân Tập). 700 bình nước suối 20 lít và 280 lốc nước, 40 can nước loại 40 lít đã được trao đến tận tay bà con.\n",
        "Nhóm An Gia Phát ở huyện Bến Lức (Long An) đã vận chuyển 4 xe tải, 3 xe ba gác chở 1.500 lốc nước về xã Tân Tập và Phước Vĩnh Tây của huyện Cần Giuộc. Bên cạnh đó, nhiều cán bộ, chiến sĩ, đoàn viên của các cơ quan đã không ngại nắng nóng, vất vả ngày đêm chung tay hỗ trợ nước ngọt miễn phí đến cho bà con vùng bị hạn mặn.\n",
        "Mỗi bà con đến nhận chỉ 2 lốc nước suối mà ai ai cũng đều trân quý. Những chai nước trao đi đã kịp thời góp phần chia sẻ khó khăn với nhân dân vùng hạn hán, xâm nhập mặn.\n",
        "Trước đó, tỉnh Long An đã công bố tình huống thiên tai xâm nhập mặn trên địa bàn thuộc rủi ro thiên tai cấp độ 4.\n",
        "\"\"\"\n",
        "chunks = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "t3KeMbpOMHeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split `documents`"
      ],
      "metadata": {
        "id": "R9MozHS7PzYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "4Icayd0CISqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[10].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LPpjfXmOdUx",
        "outputId": "2f7e59a5-060b-4744-b4c1-17ecbf8f3a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lý có thể kể đến như phân đoạn (chunking), vector hoá (embedding), lưu trữ trên cơ sở dữ liệu\n",
            "vector (vector store database). Trong quá trình truy vấn, câu hỏi người dùng sẽ được embedded\n",
            "để sẵn sàng cho các thuật toán tìm kiếm.\n",
            "•Retrieval: Quá trình tìm kiếm các đoạn tài liệu (chunks) liên quan đến câu hỏi truy vấn. Công\n",
            "tác tìm kiếm thường được thực hiện thông qua phép tìm kiếm mức độ tương đồng (similarity), so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding**"
      ],
      "metadata": {
        "id": "KFOqXqu8Np3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings()\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "QZtlw_QQNR_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embedding_model.embed_query(chunks[0])\n",
        "type(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW0jBYd5OKp4",
        "outputId": "31260866-a057-44e8-fad3-c7954c16d375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding Database**"
      ],
      "metadata": {
        "id": "b-aGgl3DEFF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/chroma_db\n",
        "from langchain_chroma import Chroma\n",
        "vectorstore = Chroma(\"example_collection2\", embedding_model)\n",
        "vectorstore_db = vectorstore.from_documents(docs, embedding=embedding_model, persist_directory=\"./chroma_db\")"
      ],
      "metadata": {
        "id": "8LS1Wpd6HBhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tạo chain 2**"
      ],
      "metadata": {
        "id": "QWjnJY4wV8Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "<|im_start|>system\n",
        "Chỉ sử dụng các thông tin sau để tạo câu trả lời. Nếu bạn không biết câu trả lời, đừng tạo câu trả lời.\n",
        "{context}<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gO7uM7fEXZJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(template=template,\n",
        "                                          input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "rbPljxziXZJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever from the vectorstore\n",
        "retriever = vectorstore_db.as_retriever(search_kwargs = {\"k\": 5})"
      ],
      "metadata": {
        "id": "YmJjU3mwYVdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "input_data = {\n",
        "    \"context\": retriever | format_docs,\n",
        "    \"question\": RunnablePassthrough()}"
      ],
      "metadata": {
        "id": "Lu9s0-FJaJYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain2 = input_data | prompt_template | llm"
      ],
      "metadata": {
        "id": "dR1oivx_XHk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_answer(text_response: str, pattern: str = r\"<\\|im_start\\|>assistant\\s*(.*)\") -> str:\n",
        "    match = re.search(pattern, text_response, re.DOTALL)\n",
        "    return match.group(1).strip() if match else text_response"
      ],
      "metadata": {
        "id": "V7x8pBjF5HY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"khái niệm reranking trong RAG là gì\""
      ],
      "metadata": {
        "id": "P88Xm0AmO6Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna4 = chain2.invoke(question)\n",
        "print(extract_answer(qna4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_VKvPkp6GPs",
        "outputId": "93339de4-346b-4cf4-f18f-b4aa18ae443d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reranking trong RAG đề cập đến quá trình sắp xếp lại các tài liệu dựa trên mức độ liên quan của chúng đối với truy vấn. Quá trình này giúp ưu tiên những tài liệu có khả năng cung cấp câu trả lời chính xác và liên quan nhất, dẫn đến cải thiện độ chính xác và chất lượng của phản hồi. Reranking có thể được thực hiện bằng cách sử dụng các thuật toán xếp hạng, chẳng hạn như re-ranking models hoặc recursive retrieval and query engine, để phân tích sự tương tác giữa tài liệu và truy vấn và phân bổ tài liệu theo thứ tự ưu tiên.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = retriever.invoke(question)\n",
        "type(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GXzB283TmCb",
        "outputId": "ab2b7e05-a085-4b00-bf90-db7095cb7783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in a:\n",
        "    print(i.page_content)\n",
        "    print(\"-----------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJZGN3nVAXn-",
        "outputId": "227a7dc6-02af-4b29-f81e-dc09189d8afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nhiệm vụ của re-ranking là đánh giá mức độ liên quan của các ngữ cảnh này và ưu tiên những cái có\n",
            "khả năng cung cấp câu trả lời chính xác và liên quan nhất. Điều này cho phép LLM ưu tiên những ngữ\n",
            "cảnh được xếp hạng cao này khi tạo ra câu trả lời, từ đó cải thiện độ chính xác và chất lượng của phản\n",
            "hồi.\n",
            "Các phương pháp re-ranking chủ yếu được chia thành hai loại:\n",
            "•Re-ranking models: những mô hình này xem xét các đặc điểm tương tác giữa tài liệu và truy vấn\n",
            "-----------------------------------------------------------\n",
            "Kết hợp với giải pháp này thường là các thuật toán xếp hạng (reranker) để phân hạng lại\n",
            "các kết quả có được từ hai phương pháp tìm kiếm.\n",
            "•Recursive Retrieval and Query Engine: Giải pháp cung cấp phương pháp phân chia\n",
            "tài liệu hiệu quả với các cấp phân chia. Cấp nhỏ để phân chia thành các đoạn tài liệu ngắn\n",
            "nhằm gia tăng khả năng tìm kiếm các nội dung tương đồng. Sau đó, cấp lớn chứa đoạn tài\n",
            "liệu dài hơn, bao chùm ngữ cảnh tốt hơn (chứa đoạn nhỏ) được trả về như kết quả tìm kiếm.\n",
            "-----------------------------------------------------------\n",
            "AI VIETNAM aivietnam.edu.vn\n",
            "Hình 17: Hình minh họa cho việc kết hợp nhiều Deep learning Model cho quá trình parse PDF\n",
            "7 Re-ranking\n",
            "Re-ranking đóng vai trò quan trọng trong RAG. Trong naive RAG, một số lượng lớn ngữ cảnh có thể\n",
            "được truy vấn, nhưng không phải tất cả đều cần thiết liên quan đến câu hỏi. Re-ranking cho phép sắp\n",
            "xếp lại và lọc các tài liệu, đặt những cái liên quan lên hàng đầu, từ đó nâng cao hiệu quả của RAG.\n",
            "-----------------------------------------------------------\n",
            "những vấn đề riêng của chúng. Những vấn đề này có thể là độc lập hoặc liên quan với nhau để \"cùng\n",
            "14\n",
            "-----------------------------------------------------------\n",
            "đó.\n",
            "•Lọc ra top k prompt có kết quả cao nhất, có thể sử dụng mô hình LLM để resample lại từ\n",
            "top k đó.\n",
            "•Lặp lại các bước trên đến khi hội tụ.\n",
            "15\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reranking**"
      ],
      "metadata": {
        "id": "mXp9SzK9gsZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = \"3xYegvpuRt4es4auipYCtnnDCSRkKBMaUR9GGcME\""
      ],
      "metadata": {
        "id": "wfmrRS4Igr8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "compressor = CohereRerank(model=\"rerank-multilingual-v3.0\", top_n=3)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n"
      ],
      "metadata": {
        "id": "FKwCel9cjZ1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_docs = compression_retriever.invoke(question)\n",
        "for i in compressed_docs:\n",
        "    print(i.page_content)\n",
        "    print(\"-----------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SIChWycijz1o",
        "outputId": "b1a31a9a-a991-4d36-b382-a68884f06c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI VIETNAM aivietnam.edu.vn\n",
            "Hình 17: Hình minh họa cho việc kết hợp nhiều Deep learning Model cho quá trình parse PDF\n",
            "7 Re-ranking\n",
            "Re-ranking đóng vai trò quan trọng trong RAG. Trong naive RAG, một số lượng lớn ngữ cảnh có thể\n",
            "được truy vấn, nhưng không phải tất cả đều cần thiết liên quan đến câu hỏi. Re-ranking cho phép sắp\n",
            "xếp lại và lọc các tài liệu, đặt những cái liên quan lên hàng đầu, từ đó nâng cao hiệu quả của RAG.\n",
            "-----------------------------------------------------------\n",
            "Nhiệm vụ của re-ranking là đánh giá mức độ liên quan của các ngữ cảnh này và ưu tiên những cái có\n",
            "khả năng cung cấp câu trả lời chính xác và liên quan nhất. Điều này cho phép LLM ưu tiên những ngữ\n",
            "cảnh được xếp hạng cao này khi tạo ra câu trả lời, từ đó cải thiện độ chính xác và chất lượng của phản\n",
            "hồi.\n",
            "Các phương pháp re-ranking chủ yếu được chia thành hai loại:\n",
            "•Re-ranking models: những mô hình này xem xét các đặc điểm tương tác giữa tài liệu và truy vấn\n",
            "-----------------------------------------------------------\n",
            "Kết hợp với giải pháp này thường là các thuật toán xếp hạng (reranker) để phân hạng lại\n",
            "các kết quả có được từ hai phương pháp tìm kiếm.\n",
            "•Recursive Retrieval and Query Engine: Giải pháp cung cấp phương pháp phân chia\n",
            "tài liệu hiệu quả với các cấp phân chia. Cấp nhỏ để phân chia thành các đoạn tài liệu ngắn\n",
            "nhằm gia tăng khả năng tìm kiếm các nội dung tương đồng. Sau đó, cấp lớn chứa đoạn tài\n",
            "liệu dài hơn, bao chùm ngữ cảnh tốt hơn (chứa đoạn nhỏ) được trả về như kết quả tìm kiếm.\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tạo lại chain với reranking**"
      ],
      "metadata": {
        "id": "5P1ZaG9dkVaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = {\n",
        "    \"context\": compression_retriever | format_docs,\n",
        "    \"question\": RunnablePassthrough()}\n",
        "\n",
        "chain3 = input_data | prompt_template | llm"
      ],
      "metadata": {
        "id": "CiizVniakajU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna4 = chain3.invoke(question)\n",
        "print(extract_answer(qna4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veOhdQxYojDh",
        "outputId": "bed1d861-7982-4d7b-dd70-d1d6f2ace4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reranking trong RAG đề cập đến quá trình sắp xếp lại các tài liệu dựa trên mức độ liên quan của chúng đối với câu hỏi. Quá trình này giúp ưu tiên những tài liệu có khả năng cung cấp câu trả lời chính xác và liên quan nhất, dẫn đến cải thiện độ chính xác và chất lượng của phản hồi. Có hai loại phương pháp reranking: reranking models và recursive retrieval and query engine. Các mô hình reranking xem xét các đặc điểm tương tác giữa tài liệu và truy vấn, trong khi giải pháp cung cấp phương pháp phân chia tài liệu và cung cấp phương pháp recursive retrieval and query engine để phân chia tài liệu hiệu quả với các cấp phân chia khác nhau.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Thử embedding với GPT4All và vectorstores FAISS**"
      ],
      "metadata": {
        "id": "37GZpvK4SKfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain_community.document_loaders.pdf import PyPDFLoader, PyPDFDirectoryLoader\n",
        "from langchain_community.document_loaders.directory import DirectoryLoader\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "from langchain_community.embeddings.gpt4all import GPT4AllEmbeddings\n",
        "def create_db_from_files(pdf_data_path):\n",
        "    # Khai báo loader để quét toàn bộ thư mục data\n",
        "    loader = DirectoryLoader(pdf_data_path, glob=\"*.pdf\", loader_cls=PyPDFLoader, show_progress=True)\n",
        "    # loader = PyPDFDirectoryLoader(pdf_data_path, glob=\"*.pdf\")\n",
        "    documents = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Embedding\n",
        "    embedding_model = GPT4AllEmbeddings(model_name='all-MiniLM-L6-v2.gguf2.f16.gguf')\n",
        "\n",
        "    # Đưa vào Faiss Vector DB\n",
        "    db = FAISS.from_documents(documents=chunks, embedding=embedding_model)\n",
        "    db.save_local(\"/content/vectorstore/db_faiss\")\n",
        "    return db\n",
        "create_db_from_files(\"/content/data\")"
      ],
      "metadata": {
        "id": "L6osaZvZPhUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "from langchain_community.embeddings.gpt4all import GPT4AllEmbeddings\n",
        "\n",
        "# Cấu hình\n",
        "vector_db_path = \"vectorstore/db_faiss\"\n",
        "\n",
        "def read_vector_db():\n",
        "    # Embedding\n",
        "    embedding_model = GPT4AllEmbeddings(model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\")\n",
        "    db = FAISS.load_local(folder_path=vector_db_path, embeddings=embedding_model, allow_dangerous_deserialization=True)\n",
        "    return db\n",
        "\n",
        "db = read_vector_db()\n",
        "\n",
        "retriever = db.as_retriever(\n",
        "    search_kwargs = {\"k\": 5},\n",
        "    return_source_documents = False\n",
        "    )"
      ],
      "metadata": {
        "id": "oS6t6x1iO5Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "<|im_start|>system\n",
        "Chỉ sử dụng các thông tin sau để tạo câu trả lời. Nếu bạn không biết câu trả lời, đừng tạo câu trả lời.\n",
        "{context}<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WkNA0cV6SGoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(template=template,\n",
        "                                          input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "ZlRRuRtjSGoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "input_data = {\n",
        "    \"context\": retriever | format_docs,\n",
        "    \"question\": RunnablePassthrough()}"
      ],
      "metadata": {
        "id": "aoGTvmnISGoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain2 = input_data | prompt_template | llm"
      ],
      "metadata": {
        "id": "MAoZ0ClGR39o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"ba vì có bao nhiêu loài thực vật bậc cao\"\n",
        "qna4 = chain2.invoke(question)\n",
        "print(extract_answer(qna4))"
      ],
      "metadata": {
        "id": "gMMDplu1Srlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = retriever.invoke(question)\n",
        "type(a)"
      ],
      "metadata": {
        "id": "kmk62ToEU3f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in a:\n",
        "    print(i.page_content)\n",
        "    print(\"-----------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "5UB9dC49U3f7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}